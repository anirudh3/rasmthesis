{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "from sklearn.cluster import KMeans\n",
    "import collections, numpy\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import re\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureData:\n",
    "\n",
    "    'Music audio features for genre classification'\n",
    "    hop_length = None\n",
    "\n",
    "    dir_trainfolder = \"/Users/anirudhmani/Prog/thesis_related/data/fma_small/000/\"\n",
    "    dir_devfolder = \"/Users/anirudhmani/Prog/thesis_related/data/fma_small/001/\"\n",
    "    dir_testfolder = \"/Users/anirudhmani/Prog/thesis_related/data/fma_small/002/\"\n",
    "#     dir_all_files = \"/Users/anirudhmani/Prog/thesis_related/data\"\n",
    "\n",
    "    train_X_preprocessed_data = 'data_train_input.npy'\n",
    "    train_Y_preprocessed_data = 'data_train_target.npy'\n",
    "    dev_X_preprocessed_data = 'data_validation_input.npy'\n",
    "    dev_Y_preprocessed_data = 'data_validation_target.npy'\n",
    "    test_X_preprocessed_data = 'data_test_input.npy'\n",
    "    test_Y_preprocessed_data = 'data_test_target.npy'\n",
    "\n",
    "    train_X = train_Y = None\n",
    "    dev_X = dev_Y = None\n",
    "    test_X = test_Y = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.hop_length = 512\n",
    "        self.timeseries_length_list = []\n",
    "\n",
    "    def load_preprocess_data(self): \n",
    "        self.trainfiles_list = self.path_to_audiofiles(self.dir_trainfolder)\n",
    "        self.devfiles_list = self.path_to_audiofiles(self.dir_devfolder)\n",
    "        self.testfiles_list = self.path_to_audiofiles(self.dir_testfolder)\n",
    "\n",
    "        all_files_list = []\n",
    "        all_files_list.extend(self.trainfiles_list)\n",
    "        all_files_list.extend(self.devfiles_list)\n",
    "        all_files_list.extend(self.testfiles_list)\n",
    "\n",
    "        #self.precompute_min_timeseries_len(all_files_list)\n",
    "        print(\"[DEBUG] total number of files: \" + str(len(self.timeseries_length_list)))\n",
    "\n",
    "        # Training set\n",
    "        self.train_X = self.extract_audio_features(self.trainfiles_list)\n",
    "        with open(self.train_X_preprocessed_data, 'wb') as f:\n",
    "            np.save(f, self.train_X)\n",
    "#         with open(self.train_Y_preprocessed_data, 'wb') as f:\n",
    "#             self.train_Y = self.one_hot(self.train_Y)\n",
    "#             np.save(f, self.train_Y)\n",
    "\n",
    "        # Validation set\n",
    "#         self.dev_X, self.dev_Y = self.extract_audio_features(self.devfiles_list)\n",
    "#         with open(self.dev_X_preprocessed_data, 'wb') as f:\n",
    "#             np.save(f, self.dev_X)\n",
    "#         with open(self.dev_Y_preprocessed_data, 'wb') as f:\n",
    "#             self.dev_Y = self.one_hot(self.dev_Y)\n",
    "#             np.save(f, self.dev_Y)\n",
    "\n",
    "        # Test set\n",
    "#         self.test_X, self.test_Y = self.extract_audio_features(self.testfiles_list)\n",
    "#         with open(self.test_X_preprocessed_data, 'wb') as f:\n",
    "#             np.save(f, self.test_X)\n",
    "#         with open(self.test_Y_preprocessed_data, 'wb') as f:\n",
    "#             self.test_Y = self.one_hot(self.test_Y)\n",
    "#             np.save(f, self.test_Y)\n",
    "\n",
    "    def load_deserialize_data(self):\n",
    "\n",
    "        self.train_X = np.load(self.train_X_preprocessed_data)\n",
    "#         self.train_Y = np.load(self.train_Y_preprocessed_data)\n",
    "\n",
    "        self.dev_X = np.load(self.dev_X_preprocessed_data)\n",
    "#         self.dev_Y = np.load(self.dev_Y_preprocessed_data)\n",
    "\n",
    "        self.test_X = np.load(self.test_X_preprocessed_data)\n",
    "#         self.test_Y = np.load(self.test_Y_preprocessed_data)\n",
    "\n",
    "    def precompute_min_timeseries_len(self, list_of_audiofiles):\n",
    "        for file in list_of_audiofiles:\n",
    "            print(\"Loading \" + str(file))\n",
    "            y, sr = librosa.load(file)\n",
    "            self.timeseries_length_list.append(math.ceil(len(y) / self.hop_length))\n",
    "\n",
    "    def extract_audio_features(self, list_of_audiofiles):\n",
    "        #timeseries_length = min(self.timeseries_length_list)\n",
    "        timeseries_length = 128 # was 128 previously\n",
    "        t_length = 12\n",
    "        data = np.zeros((len(list_of_audiofiles), timeseries_length, 13), dtype=np.float64)\n",
    "        target = []\n",
    "\n",
    "        for i, file in enumerate(list_of_audiofiles):\n",
    "            y, sr = librosa.load(file)\n",
    "            \n",
    "            mfcc = []\n",
    "            spectral_center = []\n",
    "            chroma = []\n",
    "            spectral_contrast = []\n",
    "            \n",
    "        \n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=self.hop_length, n_mfcc=13)\n",
    "#             spectral_center = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=self.hop_length)\n",
    "#             chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "#             spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=self.hop_length)\n",
    "            \n",
    "            \n",
    "\n",
    "            splits = re.split('[ .]', file)\n",
    "#             genre = re.split('[ /]', splits[1])[3]\n",
    "#             target.append(genre)\n",
    "\n",
    "\n",
    "            \n",
    "            data[i, :, 0:13] = mfcc.T[0:timeseries_length, :]\n",
    "#             data[i, :, 13:14] = spectral_center.T[0:timeseries_length, :]\n",
    "#             data[i, :, 14:26] = chroma.T[0:timeseries_length, :]\n",
    "#             data[i, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\n",
    "\n",
    "            print(\"Extracted features audio track %i of %i.\" % (i + 1, len(list_of_audiofiles)))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def one_hot(self, Y_genre_strings):\n",
    "        y_one_hot = np.zeros((Y_genre_strings.shape[0], len(self.genre_list)))\n",
    "        for i, genre_string in enumerate(Y_genre_strings):\n",
    "            index = self.genre_list.index(genre_string)\n",
    "            y_one_hot[i, index] = 1\n",
    "        return y_one_hot\n",
    "\n",
    "    def path_to_audiofiles(self, dir_folder):\n",
    "        list_of_audio = []\n",
    "        for file in os.listdir(dir_folder):\n",
    "            if file.endswith(\".mp3\"):\n",
    "                directory = \"%s/%s\" % (dir_folder, file)\n",
    "                list_of_audio.append(directory)\n",
    "        return list_of_audio\n",
    "    \n",
    "    def get_meaned(self, tracks_data, ax):     \n",
    "        return np.mean(tracks_data, axis=ax)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_features = FeatureData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] total number of files: 0\n",
      "Extracted features audio track 1 of 62.\n",
      "Extracted features audio track 2 of 62.\n",
      "Extracted features audio track 3 of 62.\n",
      "Extracted features audio track 4 of 62.\n",
      "Extracted features audio track 5 of 62.\n",
      "Extracted features audio track 6 of 62.\n",
      "Extracted features audio track 7 of 62.\n",
      "Extracted features audio track 8 of 62.\n",
      "Extracted features audio track 9 of 62.\n",
      "Extracted features audio track 10 of 62.\n",
      "Extracted features audio track 11 of 62.\n",
      "Extracted features audio track 12 of 62.\n",
      "Extracted features audio track 13 of 62.\n",
      "Extracted features audio track 14 of 62.\n",
      "Extracted features audio track 15 of 62.\n",
      "Extracted features audio track 16 of 62.\n",
      "Extracted features audio track 17 of 62.\n",
      "Extracted features audio track 18 of 62.\n",
      "Extracted features audio track 19 of 62.\n",
      "Extracted features audio track 20 of 62.\n",
      "Extracted features audio track 21 of 62.\n",
      "Extracted features audio track 22 of 62.\n",
      "Extracted features audio track 23 of 62.\n",
      "Extracted features audio track 24 of 62.\n",
      "Extracted features audio track 25 of 62.\n",
      "Extracted features audio track 26 of 62.\n",
      "Extracted features audio track 27 of 62.\n",
      "Extracted features audio track 28 of 62.\n",
      "Extracted features audio track 29 of 62.\n",
      "Extracted features audio track 30 of 62.\n",
      "Extracted features audio track 31 of 62.\n",
      "Extracted features audio track 32 of 62.\n",
      "Extracted features audio track 33 of 62.\n",
      "Extracted features audio track 34 of 62.\n",
      "Extracted features audio track 35 of 62.\n",
      "Extracted features audio track 36 of 62.\n",
      "Extracted features audio track 37 of 62.\n",
      "Extracted features audio track 38 of 62.\n",
      "Extracted features audio track 39 of 62.\n",
      "Extracted features audio track 40 of 62.\n",
      "Extracted features audio track 41 of 62.\n",
      "Extracted features audio track 42 of 62.\n",
      "Extracted features audio track 43 of 62.\n",
      "Extracted features audio track 44 of 62.\n",
      "Extracted features audio track 45 of 62.\n",
      "Extracted features audio track 46 of 62.\n",
      "Extracted features audio track 47 of 62.\n",
      "Extracted features audio track 48 of 62.\n",
      "Extracted features audio track 49 of 62.\n",
      "Extracted features audio track 50 of 62.\n",
      "Extracted features audio track 51 of 62.\n",
      "Extracted features audio track 52 of 62.\n",
      "Extracted features audio track 53 of 62.\n",
      "Extracted features audio track 54 of 62.\n",
      "Extracted features audio track 55 of 62.\n",
      "Extracted features audio track 56 of 62.\n",
      "Extracted features audio track 57 of 62.\n",
      "Extracted features audio track 58 of 62.\n",
      "Extracted features audio track 59 of 62.\n",
      "Extracted features audio track 60 of 62.\n",
      "Extracted features audio track 61 of 62.\n",
      "Extracted features audio track 62 of 62.\n"
     ]
    }
   ],
   "source": [
    "song_features.load_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_data = song_features.train_X # num of tracks * Time Length * Num of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_data = song_features.get_meaned(tracks_data, 1) # get mean of features per track, num of tracks * Num of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 13)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bog_model(data, num_clusters):\n",
    "    # all features in all songs in all artists\n",
    "    # X = get_data\n",
    "#     A = 100\n",
    "#     F = 13\n",
    "#     all_features = numpy.random.rand(A, F)\n",
    "    model = KMeans(num_clusters)\n",
    "    model.fit(data)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
